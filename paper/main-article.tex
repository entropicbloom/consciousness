% Standard Article Class Version
% For easy compilation without MDPI template
\documentclass[11pt,a4paper]{article}

% LaTeX packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{times}

% Theorem environments
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

% Title and authors
\title{\textbf{Unambiguous Representations in Neural Networks: A Relational Structure Approach to Consciousness}}

\author{
    Anonymous Author$^{1}$\\
    \small $^{1}$Affiliation\\
    \small \texttt{anonymous@example.com}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
The neural correlates of consciousness (NCC) must explain not only that consciousness arises from neural activity, but also why specific patterns of neural activity correspond to specific conscious experiences. We argue that this intentionality constraint requires conscious representations to be unambiguous: they must intrinsically specify their content without relying on an arbitrary decoding scheme. Drawing on information theory, we formalize representational ambiguity as $H(I|R)$, the conditional entropy of possible interpretations given a representation. We propose that relational structures—where meaning emerges from patterns of relationships between elements—can achieve the required unambiguity. To test this hypothesis empirically, we train neural networks on image classification tasks and demonstrate that class identity and spatial position can be decoded from network connectivity using only relational information, achieving up to 100\% accuracy through geometric structure matching. We introduce the Ambiguity-Reduction Score (ARS) to quantify how much training reduces representational ambiguity, finding that dropout regularization produces representations with near-zero ambiguity ($\text{ARS} = 1.0$) for categorical content. Our results suggest that neural networks naturally develop unambiguous relational representations, providing a measurable framework for investigating the neural basis of conscious content.
\end{abstract}

\textbf{Keywords:} consciousness; neural correlates; representation; ambiguity; information theory; neural networks; relational structure

\section*{} % spacing

% Include sections
\input{sections/01-introduction}
\input{sections/02-theory}
\input{sections/03-methods}
\input{sections/04-experiment1}
\input{sections/05-experiment2}
\input{sections/06-ambiguity-analysis}
\input{sections/07-discussion}
\input{sections/08-conclusion}

% References
\bibliographystyle{plain}
\bibliography{references}

\end{document}
