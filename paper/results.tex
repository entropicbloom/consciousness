\subsection{Experiment 1: Output Neuron Class Decoding}

\subsubsection{Learned Decoder Performance}

Output neuron class identity can be decoded from relational structure well above chance (Figure~\ref{fig:decoder-accuracy}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{decoder-validation-accuracy-training-paradigms.png}
    \caption{Decoder validation accuracy for identifying output neuron class identity across training paradigms. The decoder achieves approximately 25\% accuracy when decoding from standard backpropagation networks and 75\% when decoding from dropout networks, compared to 10\% chance level (observed when decoding from untrained networks). Error bars represent standard deviation across 5 random seeds.}
    \label{fig:decoder-accuracy}
\end{figure}

When decoding from untrained networks, the decoder achieves 10\% accuracy (chance level), validating our experimental design. For standard backpropagation networks, decoder accuracy reaches 25\% (above chance), while for dropout networks it achieves 75\%, a threefold improvement. Notably, the base networks' MNIST classification performance is nearly identical regardless of dropout (Figure~\ref{fig:mnist-accuracy}), showing that representational ambiguity is orthogonal to task performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{mnist-model-validation-accuracies.png}
    \caption{Validation accuracies of underlying MNIST models. Despite similar classification performance, dropout and standard training produce dramatically different levels of representational ambiguity.}
    \label{fig:mnist-accuracy}
\end{figure}

\subsubsection{Geometric Matching Results}

Geometric matching (directly comparing relational geometries without training) achieves even higher accuracies (Figure~\ref{fig:gram-accuracy}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{gram-matrix-decoder-performance.png}
    \caption{Geometric matching accuracies for decoding output neuron class identity across validation networks. Error bars reflect variance across network instances. Decoding from dropout networks achieves perfect 100\% accuracy with zero variance, indicating completely unambiguous relational encoding of class identity.}
    \label{fig:gram-accuracy}
\end{figure}

Geometric matching achieves perfect 100\% decoding accuracy for dropout networks with zero variance, demonstrating that their relational geometries unambiguously specify class identity. This result shows that the relational structure alone—without any information about neuron ordering or training procedure—completely determines what each neuron represents.

Figure~\ref{fig:perm-distances} reveals why geometric matching succeeds or fails. For two test networks (one trained with dropout, one trained with standard backpropagation), we compute the Frobenius distance between the reference Gram matrix and the test network's Gram matrix under all possible neuron permutations (10! = 3,628,800 permutations). If relational structure unambiguously encodes class identity, the true permutation (red dot) should yield a substantially lower distance than all incorrect permutations. For dropout networks, this is exactly what we observe: the true permutation is clearly separated from all alternatives, creating an unambiguous geometric signature. In contrast, for standard backpropagation networks, the true permutation shows only a small margin over many incorrect alternatives, making reliable identification difficult. This geometric clarity in dropout networks explains why both our learned decoder (75\% accuracy) and geometric matching (100\% accuracy) achieve their best performance on these networks.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \includegraphics[width=\textwidth]{perm_distances_no_dropout.png}
        \caption{Standard backpropagation}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[b]{0.7\textwidth}
        \includegraphics[width=\textwidth]{perm_distances_dropout.png}
        \caption{Dropout training}
    \end{subfigure}
    \caption{Frobenius distances between test and reference Gram matrices for all permutations of test network output neurons. Red dots indicate the true permutation. For standard backpropagation (a), even when the true permutation achieves the lowest distance, it does so by only a small margin. Dropout training (b) creates clear separation.}
    \label{fig:perm-distances}
\end{figure}

\subsubsection{Relational Structure Necessity}

To determine whether the entire relational geometry is necessary or if local neighborhoods suffice, we provided the decoder with only the target neuron's similarity vector to all other neurons, masking out pairwise similarities between non-target neurons. Figure~\ref{fig:target-only-output} shows that decoding accuracy drops substantially when this broader geometric context is removed. This demonstrates that a neuron's representational content cannot be determined from its local relationships alone—the decoder requires information about how the entire output population is organized relative to each other.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{target-similarity-only-output-neurons.png}
    \caption{Decoder accuracy when provided only the target neuron's local neighborhood versus full relational structure. The complete geometry is essential for accurate decoding.}
    \label{fig:target-only-output}
\end{figure}

\subsubsection{Relational Complexity Scaling}

Beyond establishing that full relational geometry is necessary, we tested whether richer relational structures systematically reduce ambiguity. We varied the number of output neurons used to compute relational structure from 2 to 10, training the decoder on Gram matrices of varying complexity.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{ablation-study-neuron-count-performance.png}
    \caption{Decoder performance as a function of relational structure complexity. The top panel shows validation accuracy (purple) versus random guessing baseline (green) for 2-10 output neurons. Error bars represent standard deviation across 5 random seeds. The bottom panel shows performance relative to random chance. The 2-neuron case achieves only random-level performance (1.0x), since asymmetric relational structures cannot exist between two points. Performance relative to chance increases systematically with neuron count, reaching 7.36x better than random for the 10-neuron model.}
    \label{fig:ablation-neuron-count}
\end{figure}

The 2-neuron case performs exactly at chance level (50\%, or 1.0x), confirming that asymmetric relational structures are essential for decoding. While the 5-neuron condition achieves the highest absolute accuracy (79.1\%), performance relative to random chance increases consistently with neuron count: the 10-neuron model performs 7.36x better than random (73.6\%). This systematic improvement demonstrates that richer relational geometries provide more constraints on possible interpretations, thereby reducing representational ambiguity. The pattern suggests that decoder accuracy should continue to improve with more complex relational structures as ambiguity decreases.

\subsubsection{Architecture Invariance}

A key question is whether relational structure depends on specific architectural choices or represents a more fundamental property. We evaluated cross-architecture transfer using geometric matching in a 3$\times$3 design, testing three different hidden layer architectures (50-50, 25-25, and 100) as both reference and test networks. Figure~\ref{fig:cross-arch} shows strong decoding performance across all architectural combinations, demonstrating that the relational geometry encoding class identity is largely architecture-invariant.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{cross_architecture_heatmap_accuracy.png}
    \caption{Cross-architecture transfer accuracy. Strong performance across different architectures demonstrates that relational structure is largely architecture-invariant.}
    \label{fig:cross-arch}
\end{figure}

\subsubsection{Dataset Discrimination}

Dataset identity (MNIST vs.\ Fashion-MNIST) can be classified from relational structure with near-perfect accuracy for dropout (99.8\% $\pm$ 0.1\%) and clearly above chance for standard training (84.3\% $\pm$ 0.8\%). This shows that dataset identity is encoded in relational geometry.

\subsection{Experiment 2: Input Neuron Spatial Position Decoding}

While digit class identity is abstract, spatial position provides a more direct connection to phenomenal properties like visual field location. We asked whether input neurons encode their pixel positions through relational structure. Figure~\ref{fig:input-distance} shows that they do: the decoder achieves $R^2 = 0.844$ when decoding from standard backpropagation networks and $R^2 = 0.695$ when decoding from dropout networks, both substantially above the untrained baseline ($R^2 \approx 0$).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{input-neuron-distance-prediction-accuracy.png}
    \caption{Decoder $R^2$ score for predicting input neuron distance from center. Unlike output neurons, decoding from standard backpropagation networks yields higher accuracy than decoding from dropout networks for this task.}
    \label{fig:input-distance}
\end{figure}

Interestingly, the pattern reverses compared to output neurons: standard backpropagation networks yield higher spatial decoding accuracy than dropout networks. The precise mechanism underlying this reversal remains unclear, though it may reflect different optimization dynamics affecting input versus output layers under dropout training. Nevertheless, as in Experiment 1, providing the decoder with the full relational structure substantially outperforms using only local neighborhoods (Figure~\ref{fig:target-only-input}), confirming that broader geometric context enhances decoding across different representational domains.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{target-similarity-only-input-pixels.png}
    \caption{Decoder performance for input neurons when using only local neighborhood versus full relational structure.}
    \label{fig:target-only-input}
\end{figure}

\subsection{Ambiguity Reduction Scores}

Table~\ref{tab:ars-scores} summarizes the Ambiguity Reduction Scores (ARS) computed from our experimental results.

\begin{table}[htbp]
    \centering
    \begin{tabular}{llcc}
        \toprule
        Experiment & Training & Performance & ARS (lower bound) \\
        \midrule
        Output (class) & Dropout & Acc = 1.000 & 1.000 \\
        Output (class) & Standard & Acc = 0.383 & 0.122 \\
        Output (class) & Untrained & Acc = 0.120 & 0.001 \\
        \midrule
        Input (position) & Dropout & $R^2 = 0.695$ & 0.419 \\
        Input (position) & Standard & $R^2 = 0.844$ & 0.654 \\
        Input (position) & Untrained & $R^2 = -0.008$ & 0.000 \\
        \bottomrule
    \end{tabular}
    \caption{Ambiguity Reduction Scores. An ARS of 1.0 indicates complete disambiguation, zero conditional entropy. Dropout networks achieve perfect disambiguation for output neuron class identity.}
    \label{tab:ars-scores}
\end{table}

The perfect ARS of 1.0 for dropout output neurons indicates $H(I|R,C) = 0$: relational structure completely determines representational content within the experimental context. Even for the more challenging spatial position task, we achieve ARS values of 0.419–0.654, demonstrating substantial ambiguity reduction.

It is important to note that these are lower bounds based on Fano's inequality and Gaussian assumptions, which may be conservative. The true ambiguity reduction could be higher. For instance, an ARS of 0.654 (input neurons, standard backpropagation) indicates that we have reduced conditional entropy by at least 65.4\% relative to maximum entropy, but the actual reduction may be greater since the bounds can be loose. Nevertheless, even these conservative estimates demonstrate substantial disambiguation, providing quantitative support for the theoretical claim that neural representations can approach the unambiguous encoding required by theories of consciousness.
