\subsection{Experiment 1: Output Neuron Class Decoding}

\subsubsection{Learned Decoder Performance}

Output neuron class identity can be decoded from relational structure well above chance (Figure~\ref{fig:decoder-accuracy}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/decoder-validation-accuracy-training-paradigms.png}
    \caption{Decoder validation accuracy across training paradigms. The decoder achieves approximately 25\% accuracy for standard backpropagation networks and 75\% accuracy for dropout networks, compared to 10\% chance level (observed for untrained networks). Error bars represent standard deviation across 5 random seeds.}
    \label{fig:decoder-accuracy}
\end{figure}

Untrained networks achieve 10\% accuracy (chance level), validating our experimental design. Standard backpropagation reaches 25\% (above chance), while dropout achieves 75\%---a threefold improvement. Notably, the underlying MNIST classifiers perform identically regardless of dropout (Figure~\ref{fig:mnist-accuracy}), showing that representational ambiguity is orthogonal to task performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{../figures/mnist-model-validation-accuracies.png}
    \caption{Validation accuracies of underlying MNIST models. Despite similar classification performance, dropout and standard training produce dramatically different levels of representational ambiguity.}
    \label{fig:mnist-accuracy}
\end{figure}

\subsubsection{Geometric Matching Results}

Geometric matching (directly comparing relational geometries without training) achieves even higher accuracies (Table~\ref{tab:gram-accuracy}).

\begin{table}[htbp]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Training Paradigm & Accuracy & Std Dev \\
        \midrule
        Untrained & 0.100 & 0.155 \\
        Standard backpropagation & 0.383 & 0.441 \\
        Dropout & \textbf{1.000} & \textbf{0.000} \\
        \bottomrule
    \end{tabular}
    \caption{Gram matrix decoding accuracies. Dropout networks achieve perfect 100\% accuracy with zero variance, indicating completely unambiguous relational encoding of class identity.}
    \label{tab:gram-accuracy}
\end{table}

Dropout networks achieve perfect 100\% accuracy with zero variance, demonstrating that their relational geometries unambiguously specify class identity. Figure~\ref{fig:perm-distances} shows why: dropout creates a substantial gap between correct and incorrect permutations, while standard backpropagation shows only small margins.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figures/perm_distances_no_dropout.png}
        \caption{Standard backpropagation}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../figures/perm_distances_dropout.png}
        \caption{Dropout training}
    \end{subfigure}
    \caption{Frobenius distances between test and reference Gram matrices for all permutations. Red dots indicate the true permutation. Dropout training creates a clear separation between correct and incorrect permutations.}
    \label{fig:perm-distances}
\end{figure}

\subsubsection{Relational Structure Necessity}

Providing only the target neuron's local neighborhood (masking other pairwise similarities) substantially reduces accuracy (Figure~\ref{fig:target-only-output}), confirming that full relational geometry is essential.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{../figures/target-similarity-only-output-neurons.png}
    \caption{Decoder accuracy when provided only the target neuron's local neighborhood versus full relational structure. The complete geometry is essential for accurate decoding.}
    \label{fig:target-only-output}
\end{figure}

\subsubsection{Architecture Invariance}

Cross-architecture transfer (Figure~\ref{fig:cross-arch}) demonstrates that relational structure transcends specific architectures.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/cross_architecture_heatmap_accuracy.png}
    \caption{Cross-architecture transfer accuracy. Strong performance across different architectures demonstrates that relational structure is largely architecture-invariant.}
    \label{fig:cross-arch}
\end{figure}

\subsubsection{Dataset Discrimination}

Dataset identity (MNIST vs.\ Fashion-MNIST) can be classified from relational structure with near-perfect accuracy for dropout (99.8\% $\pm$ 0.1\%) and clearly above chance for standard training (84.3\% $\pm$ 0.8\%). This shows that dataset identity---a major component of context $C$---is encoded in relational geometry.

\subsection{Experiment 2: Input Neuron Spatial Position Decoding}

Spatial position can be decoded from relational structure (Figure~\ref{fig:input-distance}): $R^2 = 0.844$ (standard backpropagation), $R^2 = 0.695$ (dropout), both well above untrained baseline ($R^2 \approx 0$).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/input-neuron-distance-prediction-accuracy.png}
    \caption{Decoder $R^2$ score for predicting input neuron distance from center. Unlike output neurons, standard backpropagation yields higher accuracy than dropout for this task.}
    \label{fig:input-distance}
\end{figure}

Unlike output neurons, standard backpropagation yields higher spatial decoding than dropout. As in Experiment 1, full relational structure outperforms local neighborhoods alone (Figure~\ref{fig:target-only-input}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{../figures/target-similarity-only-input-pixels.png}
    \caption{Decoder performance for input neurons when using only local neighborhood versus full relational structure.}
    \label{fig:target-only-input}
\end{figure}

\subsection{Ambiguity Reduction Scores}

Table~\ref{tab:ars-scores} summarizes the Ambiguity Reduction Scores (ARS) computed from our experimental results.

\begin{table}[htbp]
    \centering
    \begin{tabular}{llcc}
        \toprule
        Experiment & Training & Performance & ARS (lower bound) \\
        \midrule
        Output (class) & Dropout & Acc = 1.000 & 1.000 \\
        Output (class) & Standard & Acc = 0.383 & 0.122 \\
        Output (class) & Untrained & Acc = 0.120 & 0.001 \\
        \midrule
        Input (position) & Dropout & $R^2 = 0.695$ & 0.419 \\
        Input (position) & Standard & $R^2 = 0.844$ & 0.654 \\
        Input (position) & Untrained & $R^2 = -0.008$ & 0.000 \\
        \bottomrule
    \end{tabular}
    \caption{Ambiguity Reduction Scores. An ARS of 1.0 indicates complete disambiguation---zero conditional entropy. Dropout networks achieve perfect disambiguation for output neuron class identity.}
    \label{tab:ars-scores}
\end{table}

The perfect ARS of 1.0 for dropout output neurons indicates $H(I|R,C) = 0$: relational structure completely determines representational content within context, quantitatively supporting the theoretical claim that neural representations can be unambiguous.
