\begin{abstract}
Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience appears fundamentally different: a neural state corresponding to perceiving a red square cannot simultaneously encode the experience of a green square. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition by defining representational ambiguity as the conditional entropy over possible interpretations given a representation. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. Using both learned decoders and direct geometric matching, we achieve up to 100\% accuracy in identifying which digit class an output neuron represents, based solely on its relational position among other neurons. We further show that spatial position information of input neurons can be decoded from network connectivity alone. These results provide an operational framework for measuring representational ambiguity and demonstrate that neural networks can achieve the low-ambiguity representations that theoretical accounts of consciousness require. Our approach offers both a quantitative method for assessing ambiguity in neural systems and a pathway toward directly decoding representational content from relational structure.
\end{abstract}
