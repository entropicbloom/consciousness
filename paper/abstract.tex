\begin{abstract}
Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience is fundamentally different: a neural state corresponding to perceiving a red square cannot alternatively encode the experience of a green triangle. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition using information theory, defining representational ambiguity as the conditional entropy $H(I|R)$ over possible interpretations $I$ given a representation $R$. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. From relational structure alone, we achieve perfect (100\%) accuracy for dropout-trained networks and 38\% for standard backpropagation (chance: 10\%) in identifying output neuron class identity—despite identical task performance—demonstrating that representational ambiguity can arise orthogonally to behavioral accuracy. We further show that spatial position of input neurons—relevant to phenomenal properties like visual field location—can be decoded from network connectivity with $R^2$ up to 0.844. These results provide a quantitative method for measuring representational ambiguity in neural systems and demonstrate that neural networks can exhibit the low-ambiguity representations posited as necessary (though not sufficient) by theoretical accounts such as narrow representationalism and IIT.
\end{abstract}
