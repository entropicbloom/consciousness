\subsection{Implications for Consciousness Research}

We achieved perfect (100\%) decoding accuracy for dropout networks, operationalizing the claim that conscious representations must be unambiguous: the neural state itself specifies content without requiring an external decoder.

According to representationalist theories \citep{lycan2019representational,pennartz2018consciousness}, conscious experience is determined by intentional contents. Our work adds a constraint on the \emph{manner} of representation: contents must be represented unambiguously. This aligns with Chalmers' \emph{impure representationalism} \citep{chalmers2004representational}---not all representations yield consciousness, only those meeting specific conditions. We propose unambiguity as one such condition.

This constraint emerges from consciousness being intrinsic. Using IIT terminology \citep{tononi2016integrated}: consciousness is both \emph{informative} (specific content) and \emph{intrinsic} (determined by the system itself). Together, these properties imply neural substrates must unambiguously specify conscious contents.

\subsection{Context Dependence and Universal Decoders}

We measure $H(I|R,C)$ rather than $H(I|R)$, where context $C$ includes the neural network domain, dataset, architecture, and decoding target. However, this reflects practical rather than fundamental limitations:

First, dataset identity (major component of $C$) can be inferred from $R$ with 99.8\% accuracy, suggesting the $I$/$C$ distinction is artificial---context is interpretable information we chose not to decode in specific experiments.

Second, cross-architecture transfer shows that architecture is not critical---relational geometries remain consistent across layer widths, transcending implementation details.

Third, a universal decoder trained on diverse multi-modal networks could approach true $H(I|R)$, requiring only minimal context (physical domain and measurement modalities). As training diversity increases, explicit context becomes less necessary---the decoder learns to infer context from structure.

\subsection{Relational Structure and Grounding}

We focused on structural connectivity (weights) rather than functional connectivity (activity correlations) for pragmatic reasons, though functional connectivity may be more relevant theoretically. Functional connectivity grounds both elements (activations) and relations (correlations) in the same physical substrate \citep{pennartz2009identification}, representing actual rather than potential interactions.

Nevertheless, our results remain relevant: we test whether learned networks can encode information unambiguously, regardless of implementation. Moreover, structural and functional connectivity are intimately related---the former shapes the latter. Our findings suggest functional networks would exhibit similar properties.

\subsection{Training Paradigm Effects}

Dropout yields dramatically more unambiguous output representations than standard backpropagation (100\% vs.\ 38\%) despite identical task performance, showing ambiguity is orthogonal to classification accuracy. Dropout encourages distributed representations by preventing single-neuron reliance \citep{baldi2013understanding}, forcing similar classes to share features and occupy well-separated geometric positions.

Interestingly, this reverses for input neurons ($R^2 = 0.844$ standard vs.\ $R^2 = 0.695$ dropout), suggesting different optimization dynamics across layers. The key point: training paradigm substantially affects representational ambiguity independently of task performance.

\subsection{Broader Connections}

Our work connects to Huh et al.'s Platonic Representation Hypothesis \citep{huh2024platonic}: different networks converge toward similar representations when trained on similar tasks. Their k-NN kernel similarity metric correlates with our decoder accuracy, suggesting it proxies representational ambiguity.

This convergence reflects a fundamental principle: networks accurately modeling real-world distributions must capture inherent relational structure. Just as different maps of the same territory share structure, different networks modeling the same distribution develop similar relational geometries. This shared structure enables unambiguous representation---and may be necessary for conscious experience.
