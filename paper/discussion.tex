\subsection{Implications for Consciousness Research}

Our experimental results demonstrate that neural networks can encode information through relational structure in a way that allows unambiguous decoding of representational content. For dropout-trained networks, we achieved perfect (100\%) accuracy in identifying output neuron class identity from connectivity patterns alone. This operationalizes the theoretical claim that conscious representations must be unambiguous: the neural state itself specifies what it represents, without requiring an external decoder.

\subsection{What Counts as an External Decoder?}

A potential objection is that our decoding methods constitute external decoders, contradicting our claim that conscious representations require no external interpretation. This misunderstands the crucial distinction between arbitrary and structurally grounded decoders.

Consider the JPEG decoder: it is entirely conventional and arbitrary. Nothing about image bit strings determines that they should be decoded via JPEG rather than some other scheme. The decoder must be specified externally by designers who establish the encoding standard. If a different convention had been adopted, the same bit string would represent entirely different images.

Our decoders are fundamentally different. We do not impose an arbitrary mapping—instead, we \emph{infer} the decoder by examining networks themselves. Both geometric matching and learned decoders discover consistent relational patterns from network instances. Crucially, both methods converge on decoders that generalize across unseen network instances and transfer across different architectures. This generalization demonstrates that the decoder is not arbitrary but discovers structure implicitly present in the networks themselves.

The key distinction: decoders that are \emph{externally imposed conventions} versus decoders \emph{structurally necessitated by the representation itself}. A JPEG decoder could be replaced without changing the bit strings. Our decoders extract structure invariant across network instances—they discover rather than stipulate the mapping. The fact that this mapping can be learned from exemplars and applied successfully to novel instances shows it reflects genuine structural properties of the representations. An external observer must still perform computation to extract content, but the content itself is structurally determined, not conventionally assigned.

We should clarify what "unambiguous" means in this context, as it refers to informational determinacy rather than phenomenal clarity. A neural state has unambiguous representational content when that content is fixed and determinate—when the state cannot alternatively encode different contents. This leaves open whether the phenomenal character itself involves ambiguity or bistability. Consider the Necker cube: the conscious experience involves bistable perception, where the cube appears to flip between two orientations. The neural state underlying this experience does not unambiguously represent "cube from perspective A" or "cube from perspective B"—rather, it unambiguously represents "the phenomenal experience of bistable cube perception." The ambiguity is part of the determinate content, not a failure of the neural state to have determinate content. Our framework addresses semantic determinacy (what the representation is about) rather than phenomenal determinacy (whether the experience itself feels unambiguous).

How does this connect to consciousness? Recall our initial observation: if consciousness supervenes on brain states, then a neural state must unambiguously map to conscious contents. A brain state corresponding to perceiving a red square cannot alternatively encode perceiving a green square. According to representationalist theories of consciousness \citep{lycan2019representational,pennartz2018consciousness}, conscious experience is determined by intentional contents---what the brain represents. Our work adds a constraint on the \emph{manner} in which these contents must be represented: they must be represented unambiguously.

This constraint resonates with Chalmers' notion of \emph{impure representationalism} \citep{chalmers2004representational}. Impure representational properties are properties of representing certain intentional content \emph{in a certain manner}. Not all representations give rise to consciousness---only those that satisfy particular conditions. We propose that unambiguity is one such condition: intentional contents must be represented in a manner that leaves no residual ambiguity about what is being represented.

Importantly, while we have framed this work within narrow representationalism, the core constraint emerges more generally from consciousness being intrinsic. We need not commit to representationalism specifically. The same intuition can be expressed using terminology from Integrated Information Theory (IIT) \citep{tononi2016integrated}: consciousness is both \emph{informative} and \emph{intrinsic}. A conscious experience is informative in that it has specific content (seeing red rather than green), and it is intrinsic in that this content is determined by the system itself, not by external interpretation. These properties together imply that the neural substrate must unambiguously specify conscious contents.

We acknowledge that narrow representationalism itself is controversial---externalist and enactivist accounts would reject the claim that molecularly identical brains must have identical experiences. Our framework should thus be understood conditionally: \emph{if} conscious experience is determined by brain-internal states alone (whether construed representationally or otherwise), \emph{then} those states must encode content unambiguously. This work demonstrates how to measure whether this condition is met, rather than defending narrow representationalism itself.

\subsection{Context Dependence and Universal Decoders}

A potential limitation of our approach is that we measure $H(I|R,C)$ rather than $H(I|R)$, where $C$ represents context baked into the decoder. This context includes:
\begin{itemize}
    \item The neural network domain (as opposed to other physical systems)
    \item The dataset (MNIST vs.\ other visual tasks)
    \item The network architecture
    \item The choice of what to decode (class identity vs.\ other properties)
\end{itemize}

Does this conditioning on $C$ fundamentally limit our conclusions? This is a serious concern that must be addressed carefully. While we argue that $C$ can in principle be minimized, our current work should be understood as proof-of-concept rather than a complete solution to measuring conscious representation. Several lines of evidence suggest the limitations are practical rather than fundamental:

First, our dataset discrimination results show that a major component of $C$---namely, which dataset was used for training---can itself be inferred from relational structure $R$. We achieved near-perfect (99.8\%) accuracy distinguishing MNIST from Fashion-MNIST networks based solely on output layer geometry. This suggests that the distinction between content $I$ and context $C$ is somewhat artificial. Context is simply part of the interpretable information that we choose not to decode in specific experiments.

Second, our cross-architecture transfer results demonstrate that network architecture is not a critical component of context. Relational geometries remain consistent across different layer widths, allowing decoders trained on one architecture to successfully decode another. The representational content transcends implementation details.

Third, the theoretical path forward is clear: a universal decoder trained on relational structures across diverse, multi-modal neural networks could potentially approach true $H(I|R)$ rather than $H(I|R,C)$. Such a decoder would require only minimal context---perhaps just the physical domain (neural activity in our universe) and choice of measurement modalities. The more diverse the training distribution, the less explicit context is required, as the decoder learns to infer context from structure.

\subsection{Relational Structure and Grounding}

Our experiments focused on structural connectivity (synaptic weights) rather than functional connectivity (activity correlations). This choice was primarily pragmatic: it simplifies the experimental paradigm and provides stable, time-invariant relational structures. However, from a theoretical perspective, functional connectivity may be more relevant for consciousness.

The reason is that functional connectivity directly links elements and relations of the mathematical structure through the same physical quantity: neural activity. Activity correlations represent actual causal interactions occurring in the moment, whereas structural connectivity represents only the \emph{potential} for interaction. If a synapse exists but is not currently modulating neural activity, what privileges it as part of the mathematical structure of conscious experience over any other arbitrary physical quantity? Functional connectivity avoids this ambiguity by grounding both elements (neuron activations) and relations (activity correlations) in the same substrate \citep{pennartz2009identification}.

Nevertheless, our structural connectivity results remain relevant for several reasons. First, we are primarily interested in whether networks that emerge from learning input distributions can encode information unambiguously in the first place, regardless of exact physical implementation. Second, structural connectivity and functional connectivity are intimately related: the former shapes the latter, and weight patterns constrain which functional configurations are accessible. Third, for trained networks in a stable state, structural connectivity provides a time-invariant signature of the learned representational geometry.

That said, a complete test of our framework would require analyzing functional connectivity patterns during actual stimulus processing. While structural weights determine the \emph{potential} for unambiguous representation, functional dynamics during active processing would demonstrate whether this potential is realized moment-to-moment. This remains an important direction for future work, particularly in biological neural recordings where both structural and functional connectivity can be measured.

\subsection{Training Paradigm Effects}

A striking finding is that dropout training produces dramatically more unambiguous representations than standard backpropagation for output neurons (100\% vs.\ 38\% geometric matching accuracy), despite nearly identical task performance. This dissociation suggests that representational ambiguity is largely orthogonal to classification accuracy---a network can perform well while encoding information ambiguously.

Intuitively, dropout encourages distributed representations by preventing reliance on single-neuron pathways \citep{baldi2013understanding}. This forces output neurons representing similar classes to have similar input weight patterns, as they must share overlapping features. The result is a more distinctive relational geometry where each class occupies a unique, well-separated position.

We do not claim that dropout-trained networks are more conscious than standard networks, or that consciousness requires dropout specifically. Rather, dropout serves as an existence proof that training regimes can produce varying levels of representational ambiguity while maintaining task performance. The finding suggests that if biological neural systems need to minimize representational ambiguity (as our theoretical framework predicts), some form of distributed representation—whether achieved through dropout-like mechanisms or other means—may be necessary. The specific mechanism is less important than the principle: ambiguity is a dimension of neural representation that can be optimized independently of behavioral performance, at least within the performance regime we tested. Whether this independence holds more generally—particularly at the limits of task complexity or performance—remains an open question.

Interestingly, the pattern reverses for input neurons: standard backpropagation yields higher spatial position decoding accuracy than dropout ($R^2 = 0.844$ vs.\ $R^2 = 0.695$). We do not speculate extensively on the mechanism, but note that different layers may be subject to different optimization dynamics. The key point is that training paradigm substantially affects representational ambiguity in ways that are not captured by task performance metrics.

\subsection{Broader Connections}

Our core methodology—comparing relational geometries across neural network instances—is closely related to representational similarity analysis (RSA) in computational neuroscience \citep{kriegeskorte2008representational}. RSA compares similarity structures (typically across stimuli) to determine whether different systems represent information in comparable ways. This approach has successfully identified shared representational geometries between deep networks and primate visual cortex \citep{yamins2016using}, demonstrating that geometric alignment can reveal functionally equivalent representations across vastly different substrates.

Our work extends RSA in two ways. First, we compare similarity structures across network instances (trained on the same task with different initializations) rather than across stimuli within a network. Second, we use this comparison not merely to assess alignment but to decode specific representational content—recovering which neuron represents which class from geometry alone. This demonstrates that RSA-like methods can quantify representational ambiguity and potentially be applied to biological neural recordings to test consciousness-related predictions.

Our work also connects to recent findings on representation alignment across neural networks. Huh et al.'s Platonic Representation Hypothesis \citep{huh2024platonic} proposes that different neural networks converge toward similar representations when trained on similar tasks. Their mutual k-NN kernel similarity metric, originally applied to large cross-modal models, correlates with our decoder accuracy in MNIST networks. This relationship suggests that kernel similarity may serve as a proxy for representational ambiguity: higher similarity across network instances indicates more consistent, less ambiguous encoding.

This convergence is not coincidental. Both phenomena reflect the same underlying principle: neural networks that accurately model real-world distributions must capture the relational structure inherent in those distributions. Just as different maps of the same territory share common structure, different neural networks modeling the same data distribution develop similar relational geometries. This shared structure is what enables unambiguous representation---and what may be necessary for conscious experience.

\subsection{Limitations and Future Directions}

Our experiments have several important limitations. First, we focus exclusively on feedforward networks trained with supervised learning on MNIST---a simple, well-structured task. Whether similar unambiguous encoding emerges in more complex scenarios remains open: unsupervised or self-supervised learning, naturalistic datasets with richer statistical structure, recurrent networks with temporal dynamics, or deeper hierarchical architectures.

Second, our analysis examines single layers in isolation. A complete understanding would require examining how relational structures compose across layers and whether hierarchy introduces or reduces ambiguity. The relationship between Kleiner and Ludwig's mathematical structures of consciousness \citep{kleiner2024mathematical} and our empirical relational geometries deserves investigation---do their formal constraints manifest in learnable architectures?

Third, while we show that some training regimes produce more unambiguous representations than others, we lack a principled theory predicting when this occurs. Future work might connect our findings to predictive coding frameworks or work on neural manifolds and representational geometry in neuroscience.

Finally, the ultimate test would be applying this framework to biological neural data. Can we measure representational ambiguity in visual cortex recordings, and does it correlate with conscious perception? Do neural populations representing consciously perceived stimuli exhibit lower ambiguity than those processing unconscious information? These questions could provide empirical bridges between computational principles and phenomenal experience.
