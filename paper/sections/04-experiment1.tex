\section{Experiment 1: Decoding MNIST Digit Class from Relational Structure}
\label{sec:exp1}

\subsection{Setup}

Imagine an MNIST classifier where each output neuron corresponds to a digit (0-9). Normally, if you know how the network was trained, determining what a neuron represents is trivial—the neuron at position 2 represents the digit 2 because that's how we set it up.

But now suppose I scramble all the neurons in each layer. You no longer know which neuron position corresponds to which digit. The question is: \textit{can you figure out what each neuron represents purely from the network's connectivity structure?}

Our hypothesis: if the network unambiguously represents digit concepts through relational structure, then yes—each digit should occupy a unique position in the relational geometry that's recoverable even after scrambling. We test this by extracting relational structure (cosine similarities between incoming weights) and attempting to decode class identity from this structure alone.

\subsection{Results}

We trained decoders on three types of networks, varying only in how the underlying MNIST classifiers were trained:

\begin{enumerate}
    \item \textbf{Untrained (control)}: Random weights, never trained
    \item \textbf{Standard backpropagation}: Standard training with SGD
    \item \textbf{Backpropagation with dropout}: Training with 20\% dropout regularization
\end{enumerate}

Figure~\ref{fig:decoder-accuracy} shows decoder validation accuracy during training.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../figures/decoder-validation-accuracy-training-paradigms.png}
\caption{Decoder validation accuracy across training paradigms. Self-attention decoder learns to identify output neuron classes from relational structure. Three training paradigms refer to how the underlying MNIST networks were trained. Error margins show standard deviation across 5 decoder training seeds.}
\label{fig:decoder-accuracy}
\end{figure}

The results are striking:

\begin{itemize}
    \item \textbf{Untrained}: $\sim$10\% accuracy (chance level for 10 classes)—random weights have no meaningful structure
    \item \textbf{Standard backpropagation}: $\sim$25\% accuracy—well above chance, but substantial ambiguity remains
    \item \textbf{Dropout}: $\sim$75\% accuracy—dramatic improvement, showing highly distinctive relational structure
\end{itemize}

Dropout encourages distributed representations by forcing neurons to rely on population activity rather than individual pathways~\cite{baldi2013understanding}. Output neurons representing similar digits develop similar weight patterns because they must share overlapping features, creating distinctive relational geometries.

\subsection{Task Performance vs. Representational Ambiguity}

An important observation: dropout's dramatic effect on decoding accuracy ($\sim$75\% vs. $\sim$25\%) occurs despite both training methods achieving virtually identical MNIST classification performance.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../figures/mnist-model-validation-accuracies.png}
\caption{MNIST classification accuracies of the underlying networks. Both dropout and standard backpropagation achieve $\sim$97\% accuracy on the digit classification task itself, despite vastly different representational ambiguity.}
\label{fig:mnist-accuracy}
\end{figure}

Figure~\ref{fig:mnist-accuracy} shows that both training paradigms achieve $\approx 97\%$ MNIST accuracy. Yet their internal representations differ dramatically in ambiguity (25\% vs. 75\% decoding accuracy). This dissociation demonstrates that \textit{representational ambiguity is largely independent of task performance}—networks can solve the same task equally well while organizing information very differently. Dropout fundamentally changes \textit{how} information is represented, creating more distinctive relational geometries, without changing \textit{whether} the task is solved.

\subsection{Ablation: Importance of Full Relational Structure}

To verify that the decoder exploits the full relational geometry rather than just local patterns around the target neuron, we reran the experiment providing only the first row of $\mathbf{X}'$ (the target neuron's cosine similarities to all others), while masking out all pairwise similarities between non-target neurons.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../figures/target-similarity-only-output-neurons.png}
\caption{Ablation: target similarity only for output neurons. Decoder accuracy when provided with full relational structure (blue) versus only the target neuron's local neighborhood (orange). Accuracy plummets when contextual structure is removed, demonstrating that the decoder requires the full relational geometry to disambiguate class identity.}
\label{fig:ablation-target-only}
\end{figure}

Figure~\ref{fig:ablation-target-only} shows that accuracy drops substantially when contextual structure is removed. A single neuron's local neighborhood is insufficient to accurately determine its class identity; the decoder takes into account how the rest of the output population is organized to disambiguate which digit the target neuron represents. This confirms that unambiguous representation emerges from the full relational structure, not merely from local pairwise relationships.

\subsection{Ablation: Effect of Neuron Count}

To investigate how relational structure complexity affects decoding accuracy, we systematically reduced the number of output neurons available to the decoder. Figure~\ref{fig:ablation-neuron-count} shows both absolute accuracy and performance relative to random guessing across different neuron counts.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../figures/ablation-study-neuron-count-performance.png}
\caption{Ablation: neuron count performance. Top panel compares validation accuracy (purple) to random guessing baseline (green) for networks with 2-10 output neurons. Bottom panel shows relative performance gain compared to random guessing. The 2-neuron case achieves only random-level performance (1.0×), validating that asymmetric relational structures are necessary. Performance relative to random chance increases consistently with neuron count, with the 10-neuron model achieving 7.36× better than random.}
\label{fig:ablation-neuron-count}
\end{figure}

Our results confirm that asymmetric relational structures between output neurons are essential for decoding to function. The 2-neuron case performs exactly at random chance level (50\%, or 1.0×), since asymmetric relations cannot exist between only two points. While the 5-neuron condition achieves the highest absolute validation accuracy (79.1\%), performance relative to random guessing increases consistently with neuron count. The 10-neuron model performs 7.36× better than random (73.6\% absolute), suggesting that decoder accuracy should continue to improve with more complex input distributions as the ambiguity of underlying representations decreases.

\subsection{Geometric Matching: Perfect Decoding}

Having established that learned decoders can extract relational structure, we tested a more direct approach: geometric matching. Instead of training a decoder, we construct a reference geometry from 5 networks and find which permutation of a test network's neurons best aligns with this reference (by evaluating all 10! = 3,628,800 possible permutations using Frobenius distance).

Table~\ref{tab:gram-accuracy} shows the results:

\begin{table}[h]
\centering
\caption{Gram matrix decoding accuracies using 5 reference networks and 10 validation networks.}
\label{tab:gram-accuracy}
\begin{tabular}{lcc}
\toprule
\textbf{Training Paradigm} & \textbf{Accuracy} & \textbf{Std Dev} \\
\midrule
Untrained       & 0.100 & 0.155 \\
No dropout      & 0.383 & 0.441 \\
Dropout         & \textbf{1.000} & \textbf{0.000} \\
\bottomrule
\end{tabular}
\end{table}

The dropout condition achieves \textbf{perfect 100\% accuracy with zero variance}. Every single test network's true class ordering minimizes the distance to the reference geometry, with no exceptions. This means the relational structure is not just similar across networks—it is effectively identical up to permutation. The representation is \textit{completely unambiguous}: $H(I|R,C) = 0$ for digit class identity.

Geometric matching achieves higher accuracies than the learned decoder (100\% vs. 75\%) while using far fewer networks (5 references vs. 800 for training). Untrained networks perform at chance (10\%) as expected, while standard backpropagation achieves 38.3\%—much higher than the learned decoder's 25\%, but far below dropout's perfect score.

\subsubsection{Understanding Perfect Accuracy: Permutation Distance Distributions}

To understand why dropout achieves perfect accuracy while vanilla backpropagation struggles, we examined the distribution of Frobenius distances for all possible permutations. Figures~\ref{fig:perm-dist-nodropout} and~\ref{fig:perm-dist-dropout} show these distributions for representative networks.

\begin{figure}[h]
\centering
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\textwidth]{../figures/perm_distances_no_dropout.png}
\caption{No dropout}
\label{fig:perm-dist-nodropout}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\textwidth]{../figures/perm_distances_dropout.png}
\caption{Dropout}
\label{fig:perm-dist-dropout}
\end{subfigure}
\caption{Permutation distance distributions. Histograms show Frobenius distances between reference and test Gram matrices across all possible permutations. Red dots indicate the true permutation. \textbf{(a)} For networks without dropout, the true permutation has only a small margin over incorrect permutations, making it easily confused with alternatives. \textbf{(b)} For dropout networks, the true permutation shows a substantial gap from all incorrect alternatives, creating an unambiguous geometric signature.}
\label{fig:perm-dist}
\end{figure}

For networks without dropout (Figure~\ref{fig:perm-dist-nodropout}), the true permutation (red dot) has only a tiny margin over incorrect permutations, making it easily confused with alternatives. In contrast, dropout networks (Figure~\ref{fig:perm-dist-dropout}) show a substantial gap between the correct permutation and all others, creating an unambiguous geometric signature that reliably identifies the true class ordering.

This geometric clarity explains why both our learned decoder approach (75\% accuracy) and direct matching approach (100\% accuracy) achieve their best performance on dropout networks—the underlying relational structure is fundamentally more distinctive and consistent.

\subsubsection{Interpreting Perfect Accuracy: Complete Unambiguity}

The 100\% accuracy achieved by geometric matching on dropout networks deserves careful interpretation, as it represents a remarkable result both empirically and theoretically.

\textbf{What 100\% accuracy means:} Perfect decoding accuracy indicates that, across all tested network instances, the true class ordering minimizes the distance to the reference geometry with zero exceptions. This means that the relational geometry of the output layer is not merely similar across networks—it is effectively identical up to permutation. Each digit class occupies a consistent, unique position in the 10-dimensional relational space that is perfectly preserved across different random initializations and training trajectories.

\textbf{Theoretical implications:} In terms of our ambiguity framework, perfect accuracy implies $H(I|R,C) = 0$ for categorical class identity within the MNIST context. Given the relational structure of a dropout-trained network, there is zero remaining entropy about which neuron represents which digit. The representation is \textit{completely unambiguous} with respect to this interpretive task.

This is precisely what the intentionality constraint (Definition~\ref{def:intentionality}) requires: the representation intrinsically specifies its content without any remaining ambiguity. Unlike a bit string, which could mean anything depending on the decoder applied, the relational geometry unambiguously specifies ``this neuron represents the digit 3'' and no other interpretation is consistent with the structural evidence.

\textbf{Why dropout enables this:} The perfect consistency suggests that dropout fundamentally changes how networks represent categorical information. Rather than allowing each network to find an arbitrary solution within the space of functionally equivalent representations, dropout constrains learning to converge on a canonical relational structure. This structure reflects the intrinsic geometry of the MNIST distribution itself—the ways in which different digits are objectively similar or dissimilar in visual feature space.

\textbf{Contrast with standard training:} The 38\% accuracy for standard backpropagation (vs. 100\% for dropout) reveals that task performance alone is insufficient to guarantee unambiguous representations. Both training paradigms achieve $\sim$97\% MNIST classification accuracy, yet differ dramatically in representational ambiguity. Standard training finds solutions that work for the task but retain substantial ambiguity in their relational structure. Dropout finds solutions that not only work but do so through unambiguous encoding.

\textbf{Broader significance:} This result demonstrates that neural networks \textit{can} achieve the degree of unambiguity that consciousness theories require, at least for categorical representations in constrained domains. Whether biological neural networks achieve similar unambiguity, and whether this extends to richer phenomenal contents, remains to be tested. But the proof of principle is established: unambiguous relational representations are achievable in physical systems through learning.

\subsection{Cross-Architecture Transfer}

To test whether relational structure is architecture-invariant, we evaluated cross-architecture transfer for both learned decoders and geometric matching.

\subsubsection{Learned Decoder Transfer}

Figure~\ref{fig:arch-transfer-decoder} summarizes results for an unseen target architecture [100] (single hidden layer with 100 neurons).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../figures/architecture-transfer-evaluation.png}
\caption{Architecture transfer evaluation for learned decoder. A decoder trained only on [50, 50] architecture transfers above chance to [100] architecture. A decoder trained on networks with randomly sampled layer widths (25-100) achieves near-oracle performance on the held-out architecture, demonstrating architecture-independent generalization.}
\label{fig:arch-transfer-decoder}
\end{figure}

A decoder trained only on [50, 50] already transfers above chance, while a decoder trained on networks whose layer width is randomly sampled between 25 and 100 climbs almost to the self-transfer ``oracle,'' demonstrating near-architecture-independent generalization.

\subsubsection{Geometric Matching Transfer}

Figure~\ref{fig:arch-transfer-gram} shows cross-architecture transfer results for the Gram matrix matching approach.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../figures/cross_architecture_heatmap_accuracy.png}
\caption{Cross-architecture transfer heatmap for geometric matching. Each cell shows decoding accuracy when using reference networks of one architecture (y-axis) to decode test networks of another architecture (x-axis). Strong diagonal and near-diagonal performance demonstrates that the gram matrix approach maintains high accuracy across different network architectures.}
\label{fig:arch-transfer-gram}
\end{figure}

The heatmap reveals strong diagonal and near-diagonal performance, confirming that the gram matrix approach maintains high accuracy across different network architectures. This architecture-invariance suggests that the relational geometric structure reflects fundamental properties of the learned distribution rather than architectural accidents.

\subsection{Ablation: Neuron Count for Geometric Matching}

Similar to our ablation study with the learned decoder, we investigated how the number of output neurons affects the Gram matrix matching approach.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../figures/gram_neuron_ablation_plot.png}
\caption{Gram matrix decoding performance vs. number of neurons. Ablation study showing how Gram matrix matching accuracy varies with the number of output neurons available for decoding. Performance improves consistently as more relational structure becomes available.}
\label{fig:gram-neuron-ablation}
\end{figure}

Figure~\ref{fig:gram-neuron-ablation} shows that, consistent with the learned decoder results, accuracy improves as more output neurons (and thus more relational structure) become available. This reinforces the conclusion that richer relational structures enable more unambiguous representations.

\subsection{Dataset Classification from Relational Structure}

As a final test of how much information is encoded in relational structure, we trained a decoder to classify whether a network was trained on MNIST or Fashion-MNIST based solely on output layer weights with randomly permuted neurons.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../figures/dataset-classification-accuracy.png}
\caption{Dataset classification from final-layer weights. Using the same self-attention decoder with random output-neuron permutations, we classify whether a network was trained on MNIST or Fashion-MNIST. Performance is near-perfect for dropout models (0.998 ± 0.001) and clearly above chance for no dropout (0.843 ± 0.008).}
\label{fig:dataset-classification}
\end{figure}

Figure~\ref{fig:dataset-classification} shows near-perfect classification accuracy (0.998 ± 0.001) for dropout models and above-chance accuracy (0.843 ± 0.008) for standard backpropagation. This demonstrates that the relational geometry of output weights carries a dataset-specific signature, amplified by dropout. The fact that task domain can be inferred from relational structure suggests that key components of representational context are actually encoded within the representation itself.

\subsection{Can We Conclude Zero Ambiguity?}

The perfect 100\% decoding accuracy for dropout networks is remarkable—it indicates $H(I|R,C) = 0$ within our experimental context. However, can we conclude that these representations achieve truly zero ambiguity, or \textit{unconditional} $H(I|R) = 0$?

The answer is no, not yet. Our decoder has significant context baked in:

\begin{itemize}
    \item \textbf{Dataset}: The decoder knows networks were trained on MNIST, not other datasets
    \item \textbf{Architecture}: The decoder uses reference networks with known architectures
    \item \textbf{Task}: The decoder knows to predict digit class identity, not other properties
\end{itemize}

This context constrains the interpretation space. We measure $H(I|R,C)$ (entropy given representation \textit{and} context) rather than ideal $H(I|R)$ (entropy given representation alone).

However, two findings suggest this is a practical rather than fundamental limitation:

\subsubsection{Architecture Generalization}

Cross-architecture transfer (Figure~\ref{fig:arch-transfer-gram}) shows that relational structure generalizes across different network sizes. Using reference networks of one architecture to decode targets with different architectures achieves 75-100\% accuracy in most cases.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../figures/cross_architecture_heatmap_accuracy.png}
\caption{Cross-architecture transfer for geometric matching. High accuracy across different architectures (heatmap rows=reference, columns=target) shows relational structure transcends architectural specifics.}
\label{fig:arch-transfer-gram}
\end{figure}

This suggests network architecture is not a crucial component of the context—the relational structure reflects properties of the learned distribution more than architectural details.

\subsubsection{Dataset Discrimination}

Can we infer which dataset was used from relational structure alone? Figure~\ref{fig:dataset-classification} shows the answer is yes.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../figures/dataset-classification-accuracy.png}
\caption{Dataset classification from output layer weights. Using scrambled neurons, we classify whether networks were trained on MNIST vs. Fashion-MNIST with near-perfect accuracy (99.8\% for dropout, 84.3\% for standard training).}
\label{fig:dataset-classification}
\end{figure}

We achieve $\sim$100\% accuracy distinguishing MNIST from Fashion-MNIST networks based solely on relational structure. This shows that task domain—a major component of context $C$—is actually encoded in the representation $R$ itself. The distinction between ``content to decode'' ($I$) and ``context'' ($C$) is somewhat artificial: context is simply aspects of $I$ we choose not to decode in a particular experiment.

These results suggest that with richer, more diverse training and more sophisticated decoders, we could approach true $H(I|R)$ rather than just $H(I|R,C)$.
