\section{Introduction}

Understanding the neural basis of consciousness remains one of the most challenging problems in science. The search for neural correlates of consciousness (NCCs)~\cite{koch2016neural} seeks to identify which patterns of neural activity give rise to specific conscious experiences. However, identifying correlations between brain states and conscious states is not sufficient—we must also explain \textit{why} a particular pattern of neural activity corresponds to one specific experience rather than another. This is fundamentally a question about representation and meaning: how do physical patterns in the brain come to be \textit{about} something?

Consider a simple analogy: a sequence of bits encoding a JPEG image. The same bit string could, in principle, be decoded as an image, a sound file, a text document, or any other digital format. The meaning of the bit string is entirely determined by the decoding algorithm applied to it—the representation itself is \textit{ambiguous}. This observation raises a crucial question for theories of consciousness: if conscious experiences have determinate content (I see an apple, not an orange), how can this determinacy arise from neural representations that might be fundamentally ambiguous?

Traditional representational theories of consciousness~\cite{lycan2019representational} propose that conscious experiences correspond to representations in the brain, but they often leave unanswered the question of what makes these representations be \textit{about} specific contents. One might be tempted to invoke an internal decoder—some mechanism in the brain that interprets neural patterns and gives them meaning. However, this approach leads to an infinite regress: the decoder's outputs would themselves be representations requiring interpretation, recreating the homunculus fallacy~\cite{dennett1991consciousness}.

We argue that this problem can be resolved by requiring that conscious representations be \textit{unambiguous}: they must intrinsically specify both what they represent and how they represent it, without relying on an arbitrary external decoding scheme. Drawing on information theory, we formalize this intuition by defining representational ambiguity as $H(I|R)$—the conditional entropy of possible interpretations $I$ given a representation $R$. Conscious representations, we propose, must approach zero ambiguity: $H(I|R) \approx 0$.

But how can any physical system achieve such unambiguous representations? We propose that \textit{relational structures}—where the meaning of elements is determined by their relationships to other elements—provide a solution. Unlike indexical representations (such as bit strings), relational structures embed meaning in their connectivity patterns. A neural network that has learned to model the statistical structure of its input domain naturally creates such relational representations: each element's meaning is determined by its position within the larger network of relationships that mirror real-world structure.

To test this theoretical framework empirically, we conduct experiments with artificial neural networks trained on image classification tasks. We demonstrate that:

\begin{enumerate}
    \item \textbf{Class identity can be decoded from relational structure alone}: Given the output layer weights of a network trained on MNIST, we can identify which digit each output neuron represents with up to 100\% accuracy using only the relational geometry between neurons, even when neuron labels are randomly permuted.

    \item \textbf{Spatial position information is encoded relationally}: We can decode the spatial position of input pixels purely from relational patterns in network connectivity, demonstrating that even low-level sensory structure emerges as unambiguous relational representations.

    \item \textbf{Training reduces representational ambiguity}: We introduce the Ambiguity-Reduction Score (ARS), which quantifies how much the training process reduces $H(I|R)$. Networks trained with dropout regularization achieve near-perfect unambiguity (ARS = 1.0) for categorical representations.

    \item \textbf{Relational geometry is architecture-invariant}: The same relational patterns emerge across different network architectures, suggesting these structures reflect fundamental properties of the learned distribution rather than architectural accidents.
\end{enumerate}

Our work makes several contributions. Theoretically, we provide a formal framework for understanding the intentionality constraint on neural correlates of consciousness, grounded in information theory. Empirically, we demonstrate that neural networks naturally develop unambiguous relational representations, and we provide methods to measure and quantify representational ambiguity. Methodologically, we show how geometric structure matching can decode representational content with near-perfect accuracy, suggesting practical approaches for reading out the contents of neural representations.

These results have implications beyond consciousness research. They connect to recent work on representational alignment in large-scale models~\cite{huh2024platonic}, suggest new approaches to interpretability in artificial neural networks, and provide a quantitative framework for comparing representations across different systems. Most fundamentally, they demonstrate that the problem of representational ambiguity—long considered a philosophical puzzle—can be approached through rigorous experimental investigation.

The remainder of this paper is organized as follows. Section~\ref{sec:theory} develops our theoretical framework, introducing the intentionality constraint and formalizing representational ambiguity. Section~\ref{sec:methods} describes our experimental methodology. Sections~\ref{sec:exp1} and~\ref{sec:exp2} present our experiments on digit classification and spatial position decoding, respectively. Section~\ref{sec:ambiguity} introduces the Ambiguity-Reduction Score and applies it to quantify our results. Section~\ref{sec:discussion} discusses implications, limitations, and connections to related work. Section~\ref{sec:conclusion} concludes.
