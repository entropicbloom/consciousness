\section{Introduction}

We are familiar with representations of all kinds. Letters represent sounds, words represent concepts, and bit strings represent digital files. Consider a simple example: if I give you a bit string and tell you it's an image, you can apply an encoding scheme like JPEG or PNG to decode it into a picture. The same bit string could alternatively be decoded as an audio file using MP3, or as text, or any other digital format. There is nothing inherent in the bit string itself that tells you what it represents—its content is entirely dependent on an externally defined decoder. We call this kind of representation \textit{ambiguous}.

This is not a problem for bit strings or most representations we encounter in daily life. These representations are useful precisely because we can agree on decoding schemes. However, when it comes to conscious brain states, we face a fundamentally different situation.

If we assume that consciousness supervenes on brain states—a position known as narrow representationalism~\cite{lycan2019representational}—then a neural state must \textit{unambiguously} map to a set of conscious contents. For example, if a certain brain state corresponds to perceiving only a red square, that same brain state cannot alternatively encode the conscious experience of only a green square. This is because consciousness is an intrinsic property. We use the term ``consciousness'' here in the sense of \textit{phenomenal consciousness}—the ``what it is likeness'' of a certain brain state.

Here we have a fundamentally different situation than with bit strings. We cannot say that a neural state has conscious content depending on some externally defined decoder. There is only one mapping allowed: the one that corresponds to the experience that the subject actually has.

This observation raises a crucial question: how can neural representations achieve this determinacy? Traditional representational theories~\cite{lycan2019representational} propose that conscious experiences correspond to representations in the brain, but they often leave unanswered what makes these representations be \textit{about} specific contents. One might invoke an internal decoder that interprets neural patterns, but this leads to infinite regress—the decoder's outputs would themselves need interpretation, recreating the homunculus fallacy~\cite{dennett1991consciousness}.

\subsection*{Formalizing Ambiguity}

To make this notion of ambiguity more precise, we can formalize it using information theory. We define ambiguity as the conditional entropy $H(I|R)$—the entropy of all possible interpretations $I$ given a representation $R$.

For example, the representation could be a neural state, and the space of interpretations could be the space of all possible conscious experiences a human could have. A representation with \textit{high ambiguity} has a uniform (or close to uniform) distribution over possible interpretations—intuitively, we have no information about what the representation is about. On the other hand, if the probability distribution of possible interpretations given a representation is very narrow, we have a \textit{low ambiguity} representation.

This gives us a spectrum: bit strings are maximally ambiguous (they could mean anything without a decoder), while conscious brain states—given the assumption of narrow representationalism—must be unambiguous or at least substantially less ambiguous than arbitrary representations. There may be intermediate cases as well. Consider a dictionary: one might say it's ambiguous because without knowing the language, the words are meaningless. However, if we take a structuralist approach to linguistics, we might disagree. Even without prior knowledge of the language, the relationships between words shown in the dictionary could reasonably restrict the meaning of certain words. A word gets meaning by occupying a specific slot in a relational structure.

Similarly, we can think about different types of neural networks. Some might represent information in a completely ambiguous fashion, while others could unambiguously encode content. The idea is that conscious brain states would have to be on the less ambiguous end of this spectrum.

\subsection*{Testing Ambiguity in Neural Networks}

This raises an empirical question: can we demonstrate that neural networks develop unambiguous representations? To test this, we designed experiments using MNIST digit classifiers. Imagine a standard MNIST classifier with neurons corresponding to each digit (0-9). When we ask ``what does this neuron represent?'' the answer seems straightforward if you know how the network was trained—the neuron at index 2 represents the digit 2 because that's how we set up the training.

But what if you \textit{don't know} how the network was set up? What if I scramble all the neurons in each layer, destroying the correspondence between neuron position and what it represents? Could you figure out which neuron represents which digit purely from the network's structure?

Our intuition is: if the neural network unambiguously represents digit concepts, then yes, we should be able to figure this out. If not—if we cannot determine representational content purely from network connectivity—then the representations are ambiguous.

\subsection*{Approach: Relational Structure and Decoding Methods}

To operationalize this test, we extract a \textit{relational structure} from the network connectivity. This relational structure captures relationships between neurons. The idea of determining representational content from relational structure is deeply tied to structuralist ideas in the study of consciousness~\cite{lyre2022neurophenomenal,kleiner2024mathematical}. While a single neuron may not have discernable meaning on its own, it might derive meaning from the relationships it is embedded in.

For example, to determine what an output neuron represents, we extract its incoming weights and compare them to the incoming weights of other output neurons using cosine similarity. This creates a relational structure—a geometry where similar neurons are close together. We then use two complementary methods to decode representational content from this structure:

\begin{enumerate}
    \item \textbf{Geometric matching}: We create a reference relational structure from networks where we know the neuron identities, then find the alignment between reference and target that minimizes structural distance.
    \item \textbf{Machine learning decoder}: We train a decoder to predict representational content from relational structure across many network instances.
\end{enumerate}

\subsection*{Main Findings}

Our experiments reveal several key findings:

\begin{itemize}
    \item \textbf{Training reduces ambiguity}: Untrained networks perform at chance level ($\sim$10\% for 10-class classification), confirming random weights are ambiguous. Trained networks achieve much higher decoding accuracy.

    \item \textbf{Training method matters}: Networks trained with dropout regularization achieve \textit{perfect} 100\% decoding accuracy for output neurons using geometric matching, while standard backpropagation achieves only $\sim$40\%. Remarkably, both achieve nearly identical MNIST classification performance ($\sim$97\%), showing that representational ambiguity is independent of task performance.

    \item \textbf{Spatial structure is encoded relationally}: Input neuron spatial positions can be decoded with $R^2 \approx 0.84$ for standard training, demonstrating that low-level sensory structure emerges in relational representations.

    \item \textbf{Architecture invariance}: Relational structures generalize across different network architectures (varying layer sizes), suggesting they reflect properties of the learned distribution rather than architectural specifics.

    \item \textbf{Context can be inferred}: We achieve $\sim$100\% accuracy distinguishing whether networks were trained on MNIST vs. Fashion-MNIST from relational structure alone, showing that even task context is partially encoded in representations.
\end{itemize}

\subsection*{Implications}

These results demonstrate that neural networks \textit{can} achieve the unambiguous representations required for conscious content determination. The perfect decoding accuracy (ARS = 1.0) for dropout-trained networks shows that $H(I|R,C) = 0$ is achievable for categorical representations—the relational structure completely determines content with zero remaining ambiguity.

While we measure context-conditioned ambiguity $H(I|R,C)$ rather than unconditional $H(I|R)$, our results suggest this is a practical rather than fundamental limitation. The architecture invariance and dataset classification results indicate that key components of context are actually encoded within the relational structure itself.

The remainder of this paper develops the theoretical framework (Section~\ref{sec:theory}), describes the experimental methodology (Section~\ref{sec:methods}), presents detailed results for class identity (Section~\ref{sec:exp1}) and spatial position (Section~\ref{sec:exp2}) decoding, quantifies ambiguity reduction (Section~\ref{sec:ambiguity}), and discusses implications and connections to related work (Sections~\ref{sec:discussion} and~\ref{sec:conclusion}).
