\section{Theoretical Framework}
\label{sec:theory}

\subsection{Narrow Representationalism}

We can formalize the philosophical assumption underlying our work as \textit{narrow representationalism}. Representationalism holds that conscious systems instantiate representational properties—they represent objects or features of the world, and these intentional contents determine conscious experience. The ``narrow'' qualifier means these representational properties are completely determined by internal brain states, independent of external factors.

Under narrow representationalism, two molecularly identical brains would instantiate the same intentional contents and thus have the same subjective experience. If a brain state represents a red apple, that same physical state cannot alternatively represent a green square—the mapping from neural state to conscious content is \textit{unambiguous}. This stands in stark contrast to bit strings, where the same representation can mean different things depending on the decoder applied.

\subsection{The Intentionality Constraint}

This leads to a fundamental requirement for theories of consciousness:

\begin{definition}[Intentionality Constraint]
\label{def:intentionality}
The intentionality constraint on neural correlates of consciousness (NCCs) requires that an explanatory NCC must unambiguously represent the aspect of conscious experience it accounts for.
\end{definition}

\begin{corollary}
\label{cor:meaning}
Conscious representations must intrinsically specify both \textit{what} they represent and \textit{how} they represent it, without relying on an external decoder.
\end{corollary}

The intentionality constraint distinguishes conscious representations from arbitrary encodings like bit strings. When you see a red square, that experience is determined in the current moment—there is no need to compare your brain state against all possible experiences to give it meaning. Yet somehow, the meaning must be intrinsic to the representation itself.

\subsection{Formalizing Ambiguity}

We define ambiguity as the conditional entropy of possible interpretations:

\begin{equation}
\text{Ambiguity} = H(I|R)
\label{eq:ambiguity}
\end{equation}

where $H$ denotes entropy, $R$ is a representation, and $I$ represents the space of all possible interpretations. A representation with \textit{high ambiguity} has a uniform or near-uniform distribution over possible interpretations—we have little information about what it represents. A representation with \textit{low ambiguity} has a narrow distribution, concentrating probability on one or few interpretations.

For example, consider three possible interpretations $I = \{i_1, i_2, i_3\}$:
\begin{itemize}
    \item \textbf{Maximally ambiguous} (uniform): $p(i_1|R) = p(i_2|R) = p(i_3|R) = 1/3$ yields $H(I|R) = \log_2(3) \approx 1.58$ bits
    \item \textbf{Partially ambiguous}: $p(i_1|R) = 0.7, p(i_2|R) = 0.2, p(i_3|R) = 0.1$ yields $H(I|R) \approx 0.9$ bits
    \item \textbf{Unambiguous}: $p(i_1|R) = 1, p(i_2|R) = p(i_3|R) = 0$ yields $H(I|R) = 0$ bits
\end{itemize}

This gives us a spectrum of representations: bit strings are maximally ambiguous (could mean anything), while conscious brain states—given narrow representationalism—must be substantially less ambiguous, ideally approaching $H(I|R) = 0$.

\subsection{Context and Measurement}

In practice, we measure $H(I|R,C)$—the conditional entropy given both representation $R$ and context $C$. The context might include knowledge that we're dealing with neural networks trained on digit classification, or that interpretations should be digit class labels rather than arbitrary concepts. This differs from the theoretical ideal $H(I|R)$.

However, this limitation may be more practical than fundamental. Our experiments (Section~\ref{sec:exp1}) show that components of context—such as which dataset was used for training—can themselves be inferred from relational structure with near-perfect accuracy. This suggests that context and content are not sharply separable: what we treat as ``context'' may simply be aspects of $I$ we choose not to decode in a particular experiment.

For biological consciousness, the relevant context is presumably broad—``our part of the universe and our sensory modalities''—but still finite. Our framework suggests that within appropriately broad contexts, representations can achieve the required unambiguity.

\subsection{Relational Structures}

How can representations achieve unambiguity without external decoders? The key insight is that \textit{relational structures}—where elements derive meaning from their relationships to other elements—can specify content intrinsically.

Consider a simple example: encoding a 5×5 image of a square as a 25-dimensional vector (Figure~\ref{fig:pixel-encoding}a). While this \textit{looks} like a square to us, that's only because we've arranged the vector elements in a grid. The vector itself doesn't specify how it should be decoded—it's just 25 numbers. However, if we add relational information linking neighboring elements (Figure~\ref{fig:pixel-encoding}b), the representation becomes less arbitrary. Each element acquires meaning through its position in the relational structure.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../figures/pixel-encoding-relational-structure.png}
\caption{Pixel encoding and relational structure. \textbf{(a)} A binary vector visualized as a square grid. Without relational information, there's no intrinsic reason to arrange it this way. \textbf{(b)} Adding relations between neighboring elements constrains interpretation—the grid structure is now encoded in the representation itself.}
\label{fig:pixel-encoding}
\end{figure}

This idea has precedent in linguistics and consciousness research. Consider a dictionary: one might argue it's ambiguous because you need to know the language. However, from a structuralist perspective, the network of relationships between words—how they define each other—actually constrains meaning even without prior linguistic knowledge. A word derives partial meaning from occupying a specific position in the web of definitional relationships~\cite{lyre2022neurophenomenal}.

Relational structures as correlates of consciousness have been explored in depth by Kleiner and Ludwig~\cite{kleiner2024mathematical}, who argue that mathematical structures of consciousness must be intrinsically \textit{about} consciousness. Our approach complements theirs by framing the motivation in terms of representational ambiguity and information theory.

\subsection{Structuralism: Momentary vs. Potential}

An important distinction exists in how relational structures might ground meaning. Structuralism in philosophy of mind often emphasizes that experiential content is determined by relations to all \textit{potential} experiences a subject could have~\cite{lyre2022neurophenomenal}. However, for conscious experience to be determined \textit{in the current moment}, we need relational structure that is \textit{instantiated now}, not merely potential.

Our proposal focuses on this second type: conscious content is determined by the relational structure present in the current brain state. This structure may reflect or encode the space of possible experiences (learned through evolution and development), but it must be physically instantiated to determine current content.

\subsection{Physical Grounding}

For relational structures to avoid ambiguity at the physical level, the mapping from substrate to structure cannot be arbitrary. We need unambiguous correspondence between physical entities and structural elements. Key requirements include:

\begin{itemize}
    \item \textbf{Consistent physical grounding}: Elements and relations must correspond to identifiable physical entities or properties in a temporally stable way
    \item \textbf{Physical meaningfulness}: Relations between structural elements should be grounded in physical quantities that actually connect the corresponding physical entities
\end{itemize}

Ambiguity can arise at two levels: (1) the abstract mathematical structure itself (like a bit string), or (2) the physical implementation of an otherwise meaningful structure (like encoding a graph in arbitrary binary switches). Both must be addressed for truly unambiguous representation.

\subsection{Neural Networks and Learned Relational Structure}

How do neural networks relate to this framework? Consider that given a full distribution of natural images, we could potentially deduce the 2D grid structure from correlations between pixel values, even without knowing the encoding scheme a priori. A single image vector gains meaning from being embedded in the statistical structure of the distribution it's sampled from.

For conscious experience, meaning is determined in the current moment—we don't need to explicitly compare against all possible experiences. However, the brain may implicitly use distributional information learned through evolution and plasticity. By modeling the statistical structure of the natural world, neural networks embed each representation within a larger relational framework that reflects regularities in the environment.

This suggests neural networks themselves (not just their activation patterns) might constitute mathematical structures of unambiguous representation. By mirroring real-world relational structure, they avoid ambiguity at both abstract and physical levels:
\begin{itemize}
    \item \textbf{Abstract level}: The network encodes an intricate web of relations reflecting learned distributions
    \item \textbf{Physical level}: The network corresponds to actual physical nodes (neurons) and connections (synapses)
\end{itemize}

\subsection{Structural vs. Functional Connectivity}

Neural networks can instantiate relational structures through either:
\begin{enumerate}
    \item \textbf{Structural connectivity}: Presence and strength of synapses
    \item \textbf{Functional connectivity}: Statistical relationships between neural activities
\end{enumerate}

Theoretically, functional connectivity may be more appropriate for grounding conscious representations, as it captures actual ongoing relationships rather than mere possibilities. However, structural connectivity constrains and enables functional patterns. Our experiments (Sections~\ref{sec:exp1}-\ref{sec:exp2}) analyze structural connectivity as a stable proxy for the relational structure that functional connectivity would instantiate during processing.

\subsection{From Theory to Experiment}

Our theoretical framework makes a testable prediction: if neural networks develop unambiguous relational representations, then representational content should be decodable from relational structure alone, even when neuron identities are scrambled.

The experimental logic is straightforward. Consider an MNIST classifier with output neurons for digits 0-9. If I scramble the neurons (randomly permute them), can you figure out which neuron represents which digit using only the network's connectivity patterns? If representations are unambiguous, the answer should be yes—each digit occupies a unique position in the relational geometry that's consistent across different network instances learning the same distribution.

We test this by:
\begin{enumerate}
    \item Randomly permuting neurons to destroy positional information
    \item Computing relational structure (cosine similarities between neuron weights)
    \item Using decoders that see only relational information, never the same network twice
\end{enumerate}

Decoding accuracy directly relates to ambiguity:
\begin{itemize}
    \item Random performance (e.g., 10\% for 10 classes) indicates maximal ambiguity
    \item Above-chance performance indicates partial ambiguity reduction
    \item Perfect performance (100\%) indicates zero ambiguity: $H(I|R,C) = 0$
\end{itemize}

The methods section (Section~\ref{sec:methods}) provides full technical details of our experimental implementation.
