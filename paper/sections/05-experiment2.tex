\section{Experiment 2: Decoding Spatial Position from Relational Structure}
\label{sec:exp2}

\subsection{Motivation}

Experiment 1 demonstrated that abstract categorical information (digit class identity) can be decoded from relational structure. However, because of the abstract nature of class identity, the connection to phenomenal consciousness might be unintuitive—especially if one thinks of phenomenal consciousness as applying more to sensory than to abstract representations.

To address this, Experiment 2 examines whether spatial structure—a fundamental aspect of sensory experience—is also encoded relationally. This connects directly to the theoretical example of the 2D pixel grid in Section~\ref{sec:theory}. We know that input neurons represent a grid of pixels in terms of how they are used during forward passes. But is there a sense in which this spatial information is intrinsic to the network connectivity itself?

We frame this as a decoding problem: Given the first layer weight matrix with permuted columns (where rows correspond to first hidden layer neurons and columns to input neurons), can we infer positional information about input neurons? While the strictest version would be to identify exact coordinates, we also consider weaker versions such as identifying a single coordinate or distance from center.

\subsection{Methodological Considerations}

Several design choices constrain our approach:

\begin{enumerate}
    \item \textbf{Single layer analysis:} We examine only the first layer to (a) simplify the experiment and reduce degrees of freedom, and (b) preclude solutions involving passing sample inputs through the full network to test how input neurons affect outputs for different inputs.

    \item \textbf{Relational preprocessing:} Using cosine similarity preprocessing prevents trivial solutions such as inferring position from weight magnitude norms.

    \item \textbf{Note on structural vs. functional connectivity:} As in Experiment 1, we analyze structural connectivity (synaptic weights) rather than functional connectivity (activity correlations). While functional connectivity may ultimately be the proper grounding for conscious representations, structural connectivity should reflect and enable functional connectivity patterns.
\end{enumerate}

\subsection{Preliminary Evidence}

Before developing our decoder, we visualized the relational structure using UMAP dimensionality reduction on the cosine similarity matrix between input neurons of a trained network (without dropout).

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../figures/umap-input-neuron-similarity.png}
\caption{UMAP visualization of input neuron cosine similarity vectors. Two-dimensional embedding of the 784 input neurons based on their relational similarities, colored by spatial position in the 28×28 grid. Clear spatial clustering emerges, suggesting that positional information is present and decodable from relational structure alone.}
\label{fig:umap-input}
\end{figure}

Figure~\ref{fig:umap-input} shows clear spatial clustering, with input neurons organizing according to their position in the 28×28 grid. This provides preliminary evidence that positional information is indeed present in the relational structure and should be decodable.

\subsection{Task Formulation}

We cast spatial decoding as a supervised learning problem. We define a function $f(i,j)$ that extracts positional information from an input neuron's location $(i,j)$ in the 28×28 grid. The decoder's task is to predict $f(i,j)$ \textbf{given only the relational representation} of that neuron relative to all others—crucially, \textbf{without knowing} the values of $i$ or $j$ directly.

Example target functions include:
\begin{align}
f(i,j) &= \sqrt{(i - 13.5)^2 + (j - 13.5)^2} \quad \text{(distance from center)} \\
f(i,j) &= i/27 \quad \text{(normalized horizontal position)} \\
f(i,j) &= j/27 \quad \text{(normalized vertical position)}
\end{align}

For our main results, we focus on distance from center as it provides a single continuous target that captures 2D spatial structure.

\subsection{Dataset Construction}

As in Experiment 1, we generate multiple network instances with different initialization seeds. For each network, we extract the first layer weight matrix $\mathbf{W} \in \mathbb{R}^{784 \times H}$ (transposed view), where 784 corresponds to input pixels and $H$ is the first hidden layer size.

To build one training example:
\begin{enumerate}
    \item Permute the rows of $\mathbf{W}$, destroying positional information.
    \item Select one row (one input neuron) and place it first.
    \item The input $\mathbf{X}'$ to the decoder is the cosine similarity matrix computed from permuted weights (see Section~\ref{sec:methods}).
    \item The label $y = f(i,j)$ for the input neuron in the first position.
\end{enumerate}

Crucially, the decoder never sees coordinates $(i,j)$ directly—only connectivity patterns—and must infer positional information from relational structure alone.

\subsection{Results: Distance from Center Prediction}

Figure~\ref{fig:input-distance-accuracy} shows the $R^2$ scores for predicting distance from center across the three training paradigms.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../figures/input-neuron-distance-prediction-accuracy.png}
\caption{Input neuron distance prediction accuracy. $R^2$ scores for predicting pixel distance from center using relational structure alone, across three training paradigms. Error bars show standard deviation across 5 decoder training seeds.}
\label{fig:input-distance-accuracy}
\end{figure}

The results demonstrate that spatial position decoding is feasible:

\begin{itemize}
    \item \textbf{Untrained networks (control):} Achieve $R^2 \approx 0$, confirming that random weights contain no spatial information.

    \item \textbf{Standard backpropagation:} Achieve $R^2 \approx 0.84$, indicating substantial spatial structure in relational representations.

    \item \textbf{Backpropagation with dropout:} Achieve $R^2 \approx 0.70$, interestingly \textit{lower} than standard backpropagation.
\end{itemize}

The finding that dropout \textit{reduces} performance for spatial decoding contrasts sharply with Experiment 1, where dropout dramatically improved class identity decoding. This suggests a fundamental difference in how categorical vs. spatial information is encoded: dropout may enhance distinctiveness of categorical representations while making spatial representations more distributed and thus harder to decode from structural connectivity alone.

\subsection{Ablation: Importance of Full Relational Structure}

As in Experiment 1, we tested whether the decoder exploits full relational geometry or merely local neighborhoods by providing only the target neuron's cosine similarity row.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../figures/target-similarity-only-input-pixels.png}
\caption{Ablation: target similarity only for input pixels. Decoder performance when provided with full relational structure versus only the target neuron's local neighborhood. While the decoder can extract useful information from local neighborhoods alone, adding the full relational structure significantly improves performance.}
\label{fig:ablation-input}
\end{figure}

Figure~\ref{fig:ablation-input} shows that while the decoder can indeed extract useful information solely from the local neighborhood of the target neuron, adding more relations significantly improves performance. This validates our intuition that richer relational structure helps disambiguate representations—though the effect is less dramatic than in Experiment 1, suggesting that spatial information has more local character than categorical information.

\subsection{Effect of Relational Structure Size}

To assess how the size of the relational graph affects decoding accuracy, we sampled uniformly random subsets of the 784 input neurons and performed decoding on subsets of varying sizes.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../figures/varying-subset-size-input-pixels.png}
\caption{Effect of subset size on spatial decoding. Decoder $R^2$ score as a function of the number of input neurons included in the relational structure. Performance improves with larger subsets but shows diminishing returns, indicating that intentional content can be extracted even from subgraphs of the full relational structure.}
\label{fig:subset-size}
\end{figure}

Figure~\ref{fig:subset-size} reveals that while performance improves as we increase the size of the relational graph, adding more neurons yields diminishing returns beyond a certain point. This has important theoretical implications: intentional content can be extracted from neurons even when considering only a subgraph of the relational structure they are embedded in. This suggests that conscious representations might not require global integration of all neural activity, but rather a sufficiently rich local relational structure.

\subsection{Discussion}

The results of Experiment 2 demonstrate that spatial structure—a core component of sensory experience—is encoded in relational representations:

\begin{enumerate}
    \item \textbf{Spatial information is relationally encoded:} Achieving $R^2 \approx 0.84$ for position decoding shows that 2D spatial structure emerges from relational connectivity patterns, supporting the theoretical claim that relational structures can ground spatial aspects of phenomenal experience.

    \item \textbf{Different information types have different relational signatures:} The contrasting effects of dropout on categorical (improves) vs. spatial (degrades) decoding suggest that different aspects of experience may be encoded through different relational mechanisms.

    \item \textbf{Partial structures suffice:} The finding that subgraphs of relational structure can support decoding suggests that unambiguous representation does not require integrating all neural activity—a sufficiently rich local structure may suffice.

    \item \textbf{Structural connectivity reflects learned structure:} The fact that untrained networks show no spatial structure while trained networks do confirms that relational structure emerges through learning to model input distributions.
\end{enumerate}

Together with Experiment 1, these results demonstrate that neural networks develop unambiguous relational representations for both categorical and spatial information, providing empirical support for the theoretical framework developed in Section~\ref{sec:theory}.
