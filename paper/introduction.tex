Representations are ubiquitous in our world. Letters represent sounds, words represent concepts, and bit strings represent digital files. Consider a simple example: given an arbitrary bit string, one can decode it as a JPEG image by applying the appropriate encoding scheme. The same bit string could equally be decoded as an MP3 audio file using a different scheme. There is nothing inherent in the bit string itself that determines what it represents---its content depends entirely on an externally defined decoder. We call such representations \emph{ambiguous}: without specifying the decoding scheme, they could represent anything.

However, when we consider conscious brain states, the situation appears fundamentally different. If consciousness supervenes on brain states, then a neural state must unambiguously map to a set of conscious contents. For instance, if a particular brain state corresponds to perceiving a red square, that same brain state cannot alternatively encode the conscious experience of perceiving a green square. This is not to say that the neural state cannot simultaneously represent both a red square in one region of the visual field and a green square in another, but rather that the same neural activity cannot be alternatively decoded to represent different, conflicting experiences.

Why does this constraint exist for consciousness? The answer lies in consciousness being an \emph{intrinsic} property. Here we use the term consciousness in the sense of phenomenal consciousness---the ``what it is like'' quality of experience \citep{lycan2019representational}. Unlike bit strings, which acquire meaning only through external decoders, a conscious state possesses its experiential character inherently. There is only one mapping allowed: the one corresponding to the actual experience of the subject. We cannot say that a neural state has different conscious content depending on some externally defined decoder.

\subsection{Narrow Representationalism}

We can formalize this intuition through the philosophical position of narrow representationalism \citep{pennartz2009identification}. One could disagree with the assumption that two molecularly identical brains necessarily have the same conscious experience. However, narrow representationalism asserts that they would. The core idea is that conscious systems instantiate representational properties---they represent objects or features of the world. These intentional contents determine conscious experience. For example, if a red apple is represented in my brain, I will have the characteristic experience of perceiving that red apple.

The ``narrow'' aspect specifies that these representational properties are completely determined by brain states, independent of external context. If we have two molecularly identical subjects, they instantiate the same intentional contents and thus have the same subjective experience. Because the neural state completely determines intentional contents, we can equivalently say that the neural state \emph{unambiguously} represents those contents. This stands in stark contrast to bit strings, where the same representation admits multiple interpretations.

\subsection{Formalizing Ambiguity}

To make this notion of ambiguity more precise, we define it using information theory. Specifically, we define the ambiguity of a representation as the conditional entropy:
\begin{equation}
\text{Ambiguity} = H(I|R)
\end{equation}
where $I$ denotes the space of all possible interpretations and $R$ denotes the representation. A representation with high ambiguity has a uniform (or nearly uniform) distribution over possible interpretations given $R$, meaning we have little information about what the representation encodes. Conversely, a representation with low ambiguity has a narrow probability distribution over interpretations, allowing us to determine its content with high confidence.

Different types of representations occupy different positions on this ambiguity spectrum. Bit strings lie at the highly ambiguous end: without additional context, they provide no information about their intended interpretation. Conscious brain states, given the assumption of narrow representationalism, must lie at the unambiguous end---or at least significantly more unambiguous than conventional representations like bit strings. Between these extremes lie intermediate cases. For example, consider a dictionary: taken in isolation without knowledge of the language, the words are meaningless. However, from a structuralist linguistic perspective \citep{lyre2022neurophenomenal}, the relationships between words constrain their meanings. Even without prior language knowledge, the relational structure restricts possible interpretations, making the representation less ambiguous than isolated symbols.

\subsection{Relational Structures in Neural Networks}

This suggests that relational structure may be key to reducing representational ambiguity. Just as words in a dictionary acquire partial meaning from their relationships to other words, neurons in a network might derive their representational content from their position within a relational structure. Some neural networks might represent information in a maximally ambiguous fashion, while others encode information through relationships that constrain interpretation. Our central hypothesis is that conscious brain states must occupy the unambiguous end of this spectrum.

In this work, we test whether artificial neural networks can encode information unambiguously through relational structure. Specifically, we ask: given a neural network trained on a classification task, can we determine what a neuron represents based solely on its relational position among other neurons, without knowledge of how the network was constructed? If representations are truly unambiguous, the answer should be yes---the relational structure itself should specify the content.

We focus on a concrete experimental paradigm: MNIST digit classification \citep{lecun1998mnist}. In a standard MNIST classifier, output neurons are ordered such that the $n$-th neuron represents digit class $n$. But what if we scramble this ordering? Can we recover which digit each neuron represents purely from the network's connectivity structure? Moreover, can we determine which spatial positions input neurons represent based solely on their relationships to other neurons? These questions operationalize our theoretical framework and provide a quantitative measure of representational ambiguity in neural systems.
