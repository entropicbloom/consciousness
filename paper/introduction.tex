Conventional representations---letters, words, bit strings---are ambiguous: a bit string can be decoded as a JPEG image or an MP3 audio file depending on the decoding scheme applied. Nothing inherent in the representation determines its content; meaning depends entirely on an external decoder.

Conscious brain states appear fundamentally different. If consciousness supervenes on brain states, then a neural state corresponding to perceiving a red square cannot alternatively encode perceiving a green square. This constraint arises because consciousness is an \emph{intrinsic} property: phenomenal experience---the ``what it is like'' quality \citep{lycan2019representational}---is determined by the neural state itself, not by external interpretation. Unlike bit strings, conscious states cannot have different contents depending on which decoder is applied.

\subsection{Narrow Representationalism and Ambiguity}

Narrow representationalism \citep{chalmers2004representational} formalizes this constraint: conscious systems instantiate intentional contents (representations of objects or features) that determine experience, and these contents are completely determined by brain states alone. Two molecularly identical brains must instantiate identical intentional contents and thus identical experiences. This means neural states \emph{unambiguously} represent their contents---a stark contrast to bit strings, where the same representation admits multiple interpretations.

We formalize ambiguity as conditional entropy:
\begin{equation}
\text{Ambiguity} = H(I|R)
\end{equation}
where $I$ denotes possible interpretations and $R$ the representation. High ambiguity corresponds to uniform distribution over interpretations (minimal information about content); low ambiguity corresponds to concentrated probability (high confidence in content).

Representations form a spectrum: bit strings are maximally ambiguous, while conscious brain states must be unambiguous (or at least significantly more so). Our hypothesis is that relational structure may be key to reducing ambiguity---that systems can acquire determinate content from their position within a network of relations, rather than requiring external interpretation. Structuralist approaches to consciousness \citep{lyre2022neurophenomenal} suggest one avenue for how this might work: if phenomenal properties are constituted by relational structures rather than intrinsic features of individual neurons, then the geometry of neural relations could directly determine representational content.

\subsection{Experimental Approach}

We test whether neural networks can encode information unambiguously through relational structure. Given a trained MNIST classifier \citep{lecun1998mnist}, can we determine what a neuron represents based solely on its relational position among other neurons---without knowledge of network construction? If representations are truly unambiguous, relational structure alone should specify content.

We address two questions: (1) When output neurons are scrambled, can we recover which digit class each represents purely from connectivity patterns? (2) Can we determine which spatial positions input neurons represent from their relational structure? These questions operationalize our theoretical framework and provide quantitative measures of representational ambiguity.
