% Entropy Journal Paper Template
\documentclass[entropy,article,submit,moreauthors,pdftex]{Definitions/mdpi}

% MDPI internal commands
\firstpage{1}
\makeatletter
\setcounter{page}{\@firstpage}
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{1}
\pubyear{2025}
\copyrightyear{2025}

% LaTeX packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}

% Theorem environments
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

% Document information
\Title{Unambiguous Representations in Neural Networks: A Relational Structure Approach to Consciousness}

\Author{Anonymous Author$^{1,}$*}

\AuthorNames{Anonymous Author}

\address{%
$^{1}$ \quad Affiliation; anonymous@example.com\\
}

\corres{Correspondence: anonymous@example.com}

% Abstract
\abstract{%
A bit string can represent an image, audio file, or any digital format depending on which decoder is applied—the representation is fundamentally ambiguous. Conscious brain states, however, cannot be similarly ambiguous: if a brain state corresponds to perceiving a red square, it cannot alternatively represent perceiving a green square. Under narrow representationalism, neural states must unambiguously map to conscious contents. We formalize this distinction as representational ambiguity $H(I|R)$, the entropy of possible interpretations given a representation, and propose that relational structures—where elements derive meaning from their relationships—can achieve the required unambiguity. Testing this in artificial neural networks trained on MNIST, we scramble neuron identities and attempt to decode what they represent using only relational information (cosine similarities between connection weights). We achieve perfect 100\% decoding accuracy for networks trained with dropout, indicating zero ambiguity ($H(I|R,C) = 0$) for categorical representations. Remarkably, this occurs despite identical task performance ($\sim$97\% MNIST accuracy) compared to standard training (38\% decoding accuracy), showing that representational ambiguity is independent of task success. Spatial position information can be decoded with $R^2 \approx 0.84$, and task domain (MNIST vs. Fashion-MNIST) with $\sim$100\% accuracy from relational structure alone, suggesting context itself is encoded in representations. Our results demonstrate that neural networks naturally develop unambiguous relational representations through learning, providing a framework for understanding how physical systems can achieve determinate representational content.
}

% Keywords
\keyword{consciousness; neural correlates; representation; ambiguity; information theory; neural networks; relational structure}

\begin{document}

% Include sections
\input{sections/00-abstract}
\input{sections/01-introduction}
\input{sections/02-theory}
\input{sections/03-methods}
\input{sections/04-experiment1}
\input{sections/05-experiment2}
\input{sections/06-ambiguity-analysis}
\input{sections/07-discussion}
\input{sections/08-conclusion}

% References
\bibliographystyle{mdpi}
\bibliography{references}

\end{document}
